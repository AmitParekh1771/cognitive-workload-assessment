{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.decoding import Scaler, Vectorizer, CSP, cross_val_multiscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy import io, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_selection(X, y):\n",
    "    n_trials, n_channels, n_samples = X.shape\n",
    "    \n",
    "    # Calculate corrcoef of each channel pair for each trials\n",
    "    corrcoefs = np.empty((n_trials, n_channels, n_channels))\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        corrcoefs[trial] = np.corrcoef(X[trial])\n",
    "    \n",
    "    corr = np.mean(corrcoefs, axis=0)\n",
    "    \n",
    "    channel_scores = np.empty(n_channels)\n",
    "    for channel in range(n_channels):\n",
    "        channel_scores[channel] = np.sum(corr[channel] > 0.5)\n",
    "    \n",
    "    median_score = np.median(channel_scores)\n",
    "    \n",
    "    return channel_scores > median_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(X):\n",
    "    X_transformed = np.concatenate((\n",
    "        np.mean(X, axis=-1),\n",
    "        np.amax(X, axis=-1),\n",
    "        np.amin(X, axis=-1),\n",
    "        np.std(X, axis=-1),\n",
    "        stats.skew(X, axis=-1),\n",
    "        stats.kurtosis(X, axis=-1)\n",
    "    ), axis=-1)  \n",
    "    \n",
    "    return X_transformed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrebate import ReliefF\n",
    "\n",
    "class ReliefF_Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.reliefF = ReliefF(n_neighbors=10)\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.reliefF.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.reliefF.top_features_[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif\n",
    "\n",
    "class MRMR_Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        data = pd.DataFrame(X, copy=True)\n",
    "        labels = pd.Series(y, copy=True)\n",
    "        self.selected_features = mrmr_classif(X=data, y=labels, K=20, show_progress=False)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2 \n",
    "\n",
    "class Chi2_Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.chi2_selector = SelectKBest(chi2, k=20) \n",
    "        self.scaler = MinMaxScaler()\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.chi2_selector.fit(self.scaler.fit_transform(X), y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.chi2_selector.transform(self.scaler.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "class ANOVA_Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.anova_selector = SelectKBest(f_classif, k=20) \n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.anova_selector.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.anova_selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KruskalWallis_Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.p_values = []\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for idx in range(X.shape[1]):\n",
    "            _, p_value = stats.kruskal(X[:, idx], y)\n",
    "            self.p_values.append(p_value)\n",
    "\n",
    "        self.selected_features = np.argsort(self.p_values)[:20]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'A'\n",
    "\n",
    "subjects = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10',\n",
    "            '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "            '21', '22', '23', '24', '25', '26', '27', '28', '29']\n",
    "\n",
    "good_subjects = ['01', '02', '03', '09', '10',\n",
    "                '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "                '21', '23', '24', '25', '26', '27', '28', '29']\n",
    "bad_subjects = [\"04\", \"05\", \"06\", \"07\", \"08\", \"22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject_01: 66.67 ± 11.79 %\n",
      "Subject_02: 63.33 ± 17.95 %\n",
      "Subject_03: 58.33 ± 5.27 %\n",
      "Subject_04: 41.67 ± 9.13 %\n",
      "Subject_05: 48.33 ± 12.25 %\n",
      "Subject_06: 48.33 ± 13.33 %\n",
      "Subject_07: 51.67 ± 12.25 %\n",
      "Subject_08: 43.33 ± 15.28 %\n",
      "Subject_09: 76.67 ± 9.72 %\n",
      "Subject_10: 56.67 ± 14.34 %\n",
      "Subject_11: 66.67 ± 10.54 %\n",
      "Subject_12: 56.67 ± 13.33 %\n",
      "Subject_13: 68.33 ± 9.72 %\n",
      "Subject_14: 68.33 ± 12.25 %\n",
      "Subject_15: 60.00 ± 12.25 %\n",
      "Subject_16: 80.00 ± 20.14 %\n",
      "Subject_17: 61.67 ± 16.33 %\n",
      "Subject_18: 60.00 ± 15.28 %\n",
      "Subject_19: 90.00 ± 8.16 %\n",
      "Subject_20: 63.33 ± 11.30 %\n",
      "Subject_21: 65.00 ± 6.24 %\n",
      "Subject_22: 46.67 ± 6.67 %\n",
      "Subject_23: 80.00 ± 6.67 %\n",
      "Subject_24: 60.00 ± 6.24 %\n",
      "Subject_25: 78.33 ± 6.67 %\n",
      "Subject_26: 63.33 ± 8.50 %\n",
      "Subject_27: 86.67 ± 11.30 %\n",
      "Subject_28: 63.33 ± 8.50 %\n",
      "Subject_29: 56.67 ± 6.24 %\n",
      "Score: 63.10 ± 12.06 %\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for subject in subjects:\n",
    "    epochs_eeg = mne.read_epochs(f\"epochs_eeg/Dataset_{dataset}/subject_{subject}_epo.fif\", preload=True, verbose=False)\n",
    "    # epochs_fnirs = mne.read_epochs(f\"epochs_fnirs/Dataset_{dataset}/subject_{subject}_epo.fif\", preload=True, verbose=False)\n",
    "\n",
    "    epochs_eeg.crop(tmin=0, tmax=10).pick(picks=[\"eeg\"], exclude='bads')\n",
    "    # epochs_fnirs.crop(tmin=0, tmax=10).pick(picks=[\"hbo\"], exclude='bads')\n",
    "\n",
    "    X_eeg = epochs_eeg.get_data()\n",
    "    y = epochs_eeg.events[:, -1]\n",
    "\n",
    "    # X_fnirs = epochs_fnirs.get_data()\n",
    "    # y = epochs_fnirs.events[:, -1]\n",
    "\n",
    "    cv = ShuffleSplit(5, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    selector = ReliefF_Selector()\n",
    "    classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, test_idx in cv_split:\n",
    "        # selected_eeg_channels = np.where(channel_selection(X_eeg[train_idx], y[train_idx]))[0]\n",
    "        # selected_fnirs_channels = np.where(channel_selection(X_fnirs[train_idx], y[train_idx]))[0]\n",
    "\n",
    "        X_transformed = feature_extraction(X_eeg[:, :])\n",
    "        # X_transformed = feature_extraction(X_fnirs[:, :])\n",
    "\n",
    "        # X_transformed = np.concatenate((X_transformed_eeg, X_transformed_fnirs), axis=-1)\n",
    "    \n",
    "        scaler.fit(X_transformed[train_idx])\n",
    "        X_transformed = scaler.transform(X_transformed)\n",
    "        \n",
    "        # selector.fit(X_transformed[train_idx], y[train_idx])\n",
    "        # X_transformed = selector.transform(X_transformed)\n",
    "\n",
    "        classifier.fit(X_transformed[train_idx], y[train_idx])\n",
    "        y_pred = classifier.predict(X_transformed[test_idx])\n",
    "\n",
    "        score = accuracy_score(y[test_idx], y_pred)   \n",
    "\n",
    "        cv_scores.append(score)\n",
    "    \n",
    "    print(f\"Subject_{subject}: {(np.mean(cv_scores)*100):.2f} ± {(np.std(cv_scores)*100):.2f} %\")\n",
    "\n",
    "    scores.append(np.mean(cv_scores))\n",
    "\n",
    "print(f\"Score: {(np.mean(scores)*100):.2f} ± {(np.std(scores)*100):.2f} %\")\n",
    "# np.save(f\"scores/Dataset_{dataset}/WOCS_fNIRS_WOFS_LDA.npy\", scores)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive-workload-assessment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
